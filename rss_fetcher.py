"""
RSSçˆ¬å–æ¨¡å— - ä»RSSæºè·å–æ–‡ç« 
"""

import feedparser
import requests
from datetime import datetime
from utils import parse_opml, is_within_last_24_hours, format_datetime


def fetch_rss_feed(rss_url, timeout=10):
    """
    è·å–å•ä¸ªRSSæºçš„å†…å®¹
    
    Args:
        rss_url: RSSæºåœ°å€
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    
    Returns:
        feedparserè§£æåçš„å¯¹è±¡
    """
    try:
        # ä½¿ç”¨requestså…ˆè·å–å†…å®¹ï¼ˆæ›´å¥½çš„é”™è¯¯å¤„ç†ï¼‰
        response = requests.get(rss_url, timeout=timeout)
        response.raise_for_status()
        
        # ä½¿ç”¨feedparserè§£æ
        feed = feedparser.parse(response.content)
        return feed
        
    except requests.RequestException as e:
        print(f"âŒ è·å–RSSå¤±è´¥: {rss_url}")
        print(f"   é”™è¯¯: {e}")
        return None
    except Exception as e:
        print(f"âŒ è§£æRSSå¤±è´¥: {rss_url}")
        print(f"   é”™è¯¯: {e}")
        return None


def extract_articles_from_feed(feed, account_name):
    """
    ä»feedå¯¹è±¡ä¸­æå–æ–‡ç« ä¿¡æ¯
    
    Args:
        feed: feedparserè§£æåçš„å¯¹è±¡
        account_name: å…¬ä¼—å·åç§°
    
    Returns:
        æ–‡ç« åˆ—è¡¨
    """
    articles = []
    
    if not feed or not hasattr(feed, 'entries'):
        return articles
    
    for entry in feed.entries:
        try:
            # æå–åŸºæœ¬ä¿¡æ¯
            title = entry.get('title', '').strip()
            link = entry.get('link', '')
            
            # æå–å‘å¸ƒæ—¶é—´
            pub_date_str = entry.get('published', '') or entry.get('updated', '')
            
            # æå–å†…å®¹ï¼ˆå°è¯•å¤šä¸ªå¯èƒ½çš„å­—æ®µï¼‰
            content_html = ''
            if hasattr(entry, 'content') and len(entry.content) > 0:
                content_html = entry.content[0].get('value', '')
            elif hasattr(entry, 'summary'):
                content_html = entry.summary
            elif hasattr(entry, 'description'):
                content_html = entry.description
            
            # æå–æ‘˜è¦
            summary = entry.get('summary', '')
            
            # æ„é€ æ–‡ç« å¯¹è±¡
            article = {
                'title': title,
                'author': account_name,
                'url': link,
                'publish_time_raw': pub_date_str,
                'publish_time': format_datetime(pub_date_str) if pub_date_str else '',
                'content_html': content_html,
                'summary': summary
            }
            
            articles.append(article)
            
        except Exception as e:
            print(f"âš ï¸  è§£ææ–‡ç« å¤±è´¥: {entry.get('title', 'Unknown')}")
            print(f"   é”™è¯¯: {e}")
            continue
    
    return articles


def fetch_rss_articles(opml_file='wechat2rss_subscriptions.opml', filter_24h=True):
    """
    ä»OPMLä¸­çš„æ‰€æœ‰RSSæºè·å–æ–‡ç« 
    
    Args:
        opml_file: OPMLæ–‡ä»¶è·¯å¾„
        filter_24h: æ˜¯å¦åªè·å–24å°æ—¶å†…çš„æ–‡ç« 
    
    Returns:
        æ‰€æœ‰æ–‡ç« åˆ—è¡¨
    """
    print("=" * 60)
    print("ğŸš€ å¼€å§‹çˆ¬å–RSSæ–‡ç« ")
    print("=" * 60)
    
    # 1. è§£æOPMLè·å–å…¬ä¼—å·åˆ—è¡¨
    print("\nğŸ“‹ è§£æOPMLæ–‡ä»¶...")
    accounts = parse_opml(opml_file)
    print(f"âœ… æ‰¾åˆ° {len(accounts)} ä¸ªå…¬ä¼—å·")
    
    # 2. éå†æ¯ä¸ªå…¬ä¼—å·ï¼Œè·å–æ–‡ç« 
    all_articles = []
    
    for i, account in enumerate(accounts, 1):
        print(f"\n[{i}/{len(accounts)}] æ­£åœ¨çˆ¬å–: {account['name']}")
        print(f"   RSS: {account['rss_url']}")
        
        # è·å–RSSå†…å®¹
        feed = fetch_rss_feed(account['rss_url'])
        
        if not feed:
            print(f"   âš ï¸  è·³è¿‡")
            continue
        
        # æå–æ–‡ç« 
        articles = extract_articles_from_feed(feed, account['name'])
        print(f"   ğŸ“„ è·å–åˆ° {len(articles)} ç¯‡æ–‡ç« ")
        
        # è¿‡æ»¤24å°æ—¶å†…çš„æ–‡ç« 
        if filter_24h:
            filtered_articles = []
            for article in articles:
                if article['publish_time_raw'] and is_within_last_24_hours(article['publish_time_raw']):
                    filtered_articles.append(article)
            
            print(f"   â° 24å°æ—¶å†…: {len(filtered_articles)} ç¯‡")
            all_articles.extend(filtered_articles)
        else:
            all_articles.extend(articles)
    
    # 3. ç»Ÿè®¡
    print("\n" + "=" * 60)
    print(f"âœ… çˆ¬å–å®Œæˆï¼")
    print(f"   æ€»æ–‡ç« æ•°: {len(all_articles)}")
    
    # æŒ‰æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    all_articles.sort(key=lambda x: x['publish_time_raw'], reverse=True)
    
    return all_articles


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    # æµ‹è¯•çˆ¬å–
    articles = fetch_rss_articles(
        opml_file='wechat2rss_subscriptions.opml',
        filter_24h=True  # åªè·å–24å°æ—¶å†…çš„
    )
    
    # æ˜¾ç¤ºå‰3ç¯‡
    print("\n" + "=" * 60)
    print("ğŸ“° æœ€æ–°æ–‡ç« é¢„è§ˆ:")
    print("=" * 60)
    
    for i, article in enumerate(articles[:3], 1):
        print(f"\n{i}. {article['title']}")
        print(f"   ä½œè€…: {article['author']}")
        print(f"   æ—¶é—´: {article['publish_time']}")
        print(f"   é“¾æ¥: {article['url']}")
        print(f"   å†…å®¹é•¿åº¦: {len(article['content_html'])} å­—ç¬¦")
        
        # æ˜¾ç¤ºå†…å®¹å‰200å­—ç¬¦
        if article['content_html']:
            preview = article['content_html'][:200].replace('\n', ' ')
            print(f"   å†…å®¹é¢„è§ˆ: {preview}...")

