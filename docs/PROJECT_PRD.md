# 📊 项目复盘：如何更高效地梳理AI项目需求

## 🎯 项目背景

**项目名称**: 微信公众号RSS → AI选题日报系统

**原始需求**:
> 我想做一个功能，其中rss_domain=http://192.168.0.121:8081/, 所有公众号文件在wechat2rss_subscriptions.opml
> 1. 定时爬取昨天的关注的公众号的文章
> 2. 清洗数据，并且存入飞书中
> 3. 用AI帮忙分析，希望得到一个pics目录下的截图那样的文档

**最终交付**:
- ✅ RSS爬取模块
- ✅ 数据清洗模块（HTML → Markdown）
- ✅ AI分析模块（支持3种AI）
- ✅ 飞书群推送模块
- ✅ 飞书多维表格存储模块
- ✅ 三种工作模式（bitable/group/both）
- ✅ 配置检查工具
- ✅ 完整文档

**实际投入**: ~50轮对话，多次返工和调试

---

## 📈 实际开发过程

### 遇到的主要问题

1. **技术细节缺失**
   - RSS服务地址配置错误（192.168.0.121 vs localhost）
   - 不清楚wechat2rss的内部配置

2. **需求变更**
   - 最初只说"存入飞书"，后来明确要存多维表格
   - 从单一模式变为三种工作模式（bitable/group/both）

3. **技术决策不明确**
   - HTML vs Markdown（经讨论后选择Markdown）
   - 图片保留问题（最终决定保留）
   - AI选型（最终推荐DeepSeek）

4. **多次返工**
   - 函数参数不匹配（`days_ago` vs `filter_24h`）
   - 字段名不一致（`url` vs `link`, `publish_time` vs `published`）
   - 飞书权限问题（多次调试）
   - 数据格式问题（URL字段格式、空值处理）

---

## 💡 如果重新来过：更好的需求梳理方式

### 第1步：先问5个核心问题

不要直接说"我想做XXX"，而是先回答这5个问题：

```markdown
## 项目启动清单

### Q1: 最核心的价值是什么？
❓ 问题：这个项目要解决什么问题？
💡 示例：让AI帮我筛选和总结公众号文章，节省每天2小时刷公众号的时间

### Q2: 数据来源是什么状态？
❓ 问题：数据从哪来？什么格式？有什么问题？
💡 示例：
- 已有 wechat2rss 服务（Docker，监听localhost:8081）
- 已有20个公众号的RSS订阅（OPML文件）
- 文章格式：HTML，含广告，质量参差不齐
- RSS Feed返回格式：XML（feedparser可解析）

### Q3: 最终产物要什么样？
❓ 问题：期望的输出是什么？有参考吗？
💡 示例：（附上截图 pics/image.png）
- 数据统计（文章数、公众号数）
- 3个选题灵感（可商业化的角度）
- 3篇深度阅读推荐（实操价值高的）

### Q4: 数据要存哪里？优先级如何？
❓ 问题：多个输出渠道时，哪个最重要？
💡 示例：
- P0（必须）：AI报告推送到飞书群（每天看一眼）
- P1（很重要）：原始文章存飞书表格（方便后续查找）
- P2（可选）：本地备份JSON

### Q5: 预算和时间约束？
❓ 问题：成本和时间限制是什么？
💡 示例：
- 运行频率：每天1次
- AI成本：能接受每月¥5以内
- 开发时间：希望2周内上线MVP
```

---

### 第2步：拆解成独立可验证的模块

**核心原则**: 每个模块都能单独测试和验证

```markdown
## 模块拆解清单

### 模块1：数据采集 🔴 核心
**输入**: RSS Feed URL
**输出**: 文章列表（JSON）
**关键字段**: title, author, link, published, summary, content
**验证标准**: 
- ✅ 能获取到昨天发布的所有文章
- ✅ 字段完整，无缺失
**测试方法**: 先用curl测试RSS返回，再写Python解析

---

### 模块2：数据清洗 🔴 核心
**输入**: 原始HTML文章
**输出**: 纯净的文本/Markdown
**处理步骤**:
- 去除 <script>, <style> 标签
- 去除广告（正则匹配）
- HTML → Markdown（保留结构）
- 去重（基于URL/标题）
- 过滤低质量（字数<500）
**验证标准**: 
- ✅ 无广告内容
- ✅ 保留正文结构
- ✅ 图片链接保留
**测试方法**: 对比1篇文章清洗前后的效果

---

### 模块3：AI分析 🔴 核心
**输入**: 清洗后的文章（Markdown格式）
**输出**: 结构化报告（JSON）
**AI任务**:
1. 数据统计
2. 生成3个选题灵感
3. 推荐3篇深度阅读
**验证标准**: 
- ✅ 返回格式符合截图要求
- ✅ 推荐的文章确实有价值（人工抽查）
- ✅ 成本可控（单次<¥0.1）
**测试方法**: 先用网页版Claude测试提示词，再集成API

---

### 模块4：飞书群推送 🟡 重要但非紧急
**输入**: AI报告（JSON）
**输出**: 飞书群消息（富文本）
**技术要点**:
- 获取 tenant_access_token
- 构建富文本消息（标题、列表、链接）
- 错误处理
**验证标准**: 
- ✅ 格式美观
- ✅ 链接可点击
- ✅ 权限正确
**测试方法**: 先发送纯文本，再优化格式

---

### 模块5：多维表格存储 🟢 可选
**输入**: 清洗后的文章
**输出**: 飞书多维表格记录
**字段映射**:
| 字段名 | 来源 | 类型 |
|--------|------|------|
| 标题 | title | 单行文本 |
| 作者 | author | 单行文本 |
| 链接 | url/link | URL |
| 发布时间 | publish_time | 日期 |
| 摘要 | summary | 多行文本 |
| 内容 | content_markdown | 多行文本 |
| 字数 | word_count | 数字 |
**验证标准**: 
- ✅ 字段完整，无空值
- ✅ 时间格式正确
- ✅ URL可点击
**测试方法**: 先插入1条记录，检查所有字段
```

---

### 第3步：明确分阶段实施计划

```markdown
## 第1周：MVP（最小可行产品）

目标：能生成AI报告，手动查看

### Day 1-2: 模块1（数据采集）
- [ ] 用 curl 测试 RSS Feed 返回格式
- [ ] 编写 rss_fetcher.py
- [ ] 测试脚本：test_fetch.py
- [ ] 输出：test_output.json（人工检查）

### Day 3-4: 模块2（数据清洗）
- [ ] 选择 HTML → Markdown 库（markdownify）
- [ ] 实现去广告逻辑
- [ ] 实现去重和过滤
- [ ] 测试：对比1篇文章清洗前后效果
- [ ] 输出：cleaned_output.json

### Day 5-6: 模块3（AI分析）
- [ ] 编写 AI 提示词（参考截图）
- [ ] 用 Claude 网页版测试提示词
- [ ] 选择 AI 提供商（考虑成本）
- [ ] 集成 API
- [ ] 输出：ai_report.json

### Day 7: 整合测试
- [ ] main.py 串联3个模块
- [ ] 本地运行完整流程
- [ ] 人工验证结果质量
- [ ] 成本估算

---

## 第2周：完善功能

### Day 8-9: 模块4（飞书推送）
- [ ] 创建飞书应用，获取凭证
- [ ] 测试：发送纯文本消息
- [ ] 优化：富文本格式
- [ ] 测试：在群中查看效果

### Day 10-11: 模块5（数据存储，可选）
- [ ] 创建飞书多维表格
- [ ] 配置字段（8个必需字段）
- [ ] 开通应用权限
- [ ] 添加应用为表格协作者
- [ ] 实现批量插入
- [ ] 测试：检查所有字段

### Day 12-14: 优化和部署
- [ ] 配置检查工具（check_config.py）
- [ ] 错误处理和日志
- [ ] 定时任务（crontab）
- [ ] 文档编写
- [ ] 成本监控
```

---

### 第4步：提供完整的上下文信息

```markdown
## 技术栈现状

### 已有基础设施
- ✅ Docker Desktop（已安装）
- ✅ wechat2rss 服务（已运行，docker-compose.yml）
- ✅ Python 3.8+（已安装）
- ⚠️ wechat2rss 监听 localhost:8081（不是IP地址）

### API账号情况
- ✅ 飞书：有企业账号，可创建应用
- ❌ Claude API：需要单独付费（已有Pro不能用于API）
- ❓ OpenAI/DeepSeek：待决定

### 技术偏好和约束
**偏好**:
- ✅ 不想用数据库（SQLite也不要）
- ✅ 希望文件结构简单
- ✅ 配置集中在一个 config.py
- ✅ 有详细的错误提示

**约束**:
- 💰 AI成本 < ¥5/月
- ⏰ 每天运行1次（早上8点）
- 📊 至少保留3天的数据

### 不确定的点（需要帮助决策）
❓ HTML转文本 vs 转Markdown？
→ 建议：Markdown（保留结构，AI理解更好）

❓ 图片要不要保留？
→ 建议：保留链接（AI可统计图片数量判断价值密度）

❓ AI选哪家（Claude/OpenAI/DeepSeek）？
→ 建议：DeepSeek（便宜，¥0.03/次）

❓ "存入飞书"是指？
→ 需明确：群消息？多维表格？文档？

❓ 需要定时任务吗？
→ 建议：crontab（简单可靠）
```

---

### 第5步：定义明确的验收标准

```markdown
## 验收标准

### P0：必须满足（MVP）
- ✅ 每天能自动爬取昨天的文章
- ✅ 数据清洗后无广告，保留正文
- ✅ AI报告格式符合截图要求
- ✅ 飞书群能收到推送
- ✅ 成本 < ¥5/月
- ✅ 出错有清晰提示

### P1：期望满足（完整版）
- ✅ 文章存入飞书表格，方便检索
- ✅ 配置检查工具，自动诊断问题
- ✅ 运行日志，方便排查
- ✅ 配置一次后，无需每天维护
- ✅ 详细文档（README + USAGE_GUIDE）

### P2：锦上添花（扩展）
- 📊 周报/月报汇总
- 🔔 特别重要的文章单独提醒
- 📈 追踪热门话题趋势
- 🔍 关键词搜索
- 📥 导出功能
```

---

## 🔍 对比分析：实际 vs 理想

| 维度 | 实际过程 | 理想过程 | 差异 |
|------|---------|---------|------|
| **对话轮数** | ~50轮 | ~20轮 | 60%减少 |
| **返工次数** | 7次 | 1-2次 | 70%减少 |
| **开发模式** | 边做边改 | 模块化验证 | 线性 vs 迭代 |
| **需求变更** | 3次重大变更 | 基本无变更 | 稳定性提升 |
| **技术决策** | 事后调整 | 事前明确 | 风险前置 |
| **文档完整度** | 事后补充 | 同步编写 | 质量更高 |

### 实际过程（我们经历的）

```
原始需求（模糊）
    ↓
AI直接设计方案
    ↓
开始写代码
    ↓
❌ RSS地址配置错误（192.168.0.121 → localhost）
    ↓
修复，继续
    ↓
❌ 函数参数不匹配（days_ago vs filter_24h）
    ↓
修复，继续
    ↓
❌ 字段名不一致（url vs link）
    ↓
修复，继续
    ↓
💡 需求变更：要加多维表格
    ↓
重新开发模块5
    ↓
❌ 飞书权限问题（多次调试）
    ↓
修复，继续
    ↓
❌ URL字段格式错误
    ↓
修复，继续
    ↓
❌ 空值问题
    ↓
修复，继续
    ↓
💡 需求变更：要三种工作模式
    ↓
重构 main.py
    ↓
✅ 最终完成
```

**总耗时**: ~50轮对话，7次返工

---

### 理想过程（如果重来）

```
5个核心问题
    ↓
明确：目标、数据源、产物、优先级、约束
    ↓
模块拆解（5个独立模块）
    ↓
模块1测试
    ↓
✅ 通过（test_fetch.py）
    ↓
模块2测试
    ↓
✅ 通过（test_clean.py）
    ↓
模块3测试
    ↓
✅ 通过（test_ai.py）
    ↓
模块4测试
    ↓
✅ 通过（test_feishu_push.py）
    ↓
整合（main.py）
    ↓
✅ 完整流程通过
    ↓
扩展：模块5（多维表格）
    ↓
✅ 完成
```

**预计耗时**: ~20轮对话，线性推进

---

## 🎯 关键差异分析

### ❌ 原始需求的5个问题

#### 1. 目标模糊
**问题**: "存入飞书中"
**模糊点**: 
- 存哪里？（群消息/表格/文档）
- 什么格式？（JSON/富文本/HTML）
- 优先级？（哪个必须有？）

**改进**:
```markdown
✅ 明确说明：
- P0：AI报告推送到飞书群
- P1：原始文章存多维表格
- P2：本地备份（可选）
```

---

#### 2. 技术细节缺失
**问题**: "rss_domain=http://192.168.0.121:8081"
**缺失点**:
- 为什么是IP而不是localhost？
- Docker内部监听什么地址？
- 是否测试过这个地址可访问？

**改进**:
```markdown
✅ 提供完整信息：
- wechat2rss 运行在 Docker 容器
- docker-compose.yml 中配置为 localhost:8081
- 已用 curl 测试，确认可访问
- 附上 docker ps 输出
```

---

#### 3. 没有优先级
**问题**: 三个功能并列（爬取、清洗、AI分析）
**模糊点**:
- 哪个最重要？
- 能不能分阶段做？
- MVP是什么？

**改进**:
```markdown
✅ 明确优先级：
- 第1周MVP：爬取 + 清洗 + AI分析（本地验证）
- 第2周完善：飞书推送 + 多维表格
- 第3周优化：定时任务 + 错误处理
```

---

#### 4. 缺少验收标准
**问题**: "希望得到一个pics目录下的截图那样的文档"
**模糊点**:
- 只要格式像就行？
- 还是内容质量也要验证？
- 怎么算"成功"？

**改进**:
```markdown
✅ 明确验收标准：
- 格式：完全符合截图（JSON结构）
- 质量：推荐的文章人工抽查，80%确实有价值
- 成本：单次运行 < ¥0.1
- 速度：5分钟内完成
```

---

#### 5. 未提供上下文
**问题**: 只说"定时爬取"
**缺失点**:
- 现有系统是什么状态？
- 技术栈偏好？
- 遇到过什么坑？

**改进**:
```markdown
✅ 提供完整上下文：
- 已有 Docker Desktop，熟悉基本操作
- Python 3.8+，熟悉虚拟环境
- 不想用数据库（之前踩过坑）
- 飞书账号已有，但不熟悉开放平台
- 预算有限，希望用最便宜的AI
```

---

## 📝 通用需求梳理模板

### 复制这个模板，填写后再开始项目

```markdown
# 项目需求文档

## 1️⃣ 一句话目标
[用一句话说清楚要解决什么问题]

示例：让AI帮我筛选公众号文章，节省每天2小时刷信息的时间

---

## 2️⃣ 现状和痛点

### 现在怎么做的？
[描述当前的工作流程]

### 有什么问题？
[列出具体的痛点]

### 理想情况是什么样？
[描述期望的状态]

---

## 3️⃣ 数据流分析

### 数据来源
- 来源：[API/文件/数据库/网页]
- 格式：[JSON/XML/HTML/CSV]
- 频率：[实时/每天/每周]
- 质量：[完整/有缺失/有噪音]

### 数据处理
- 清洗需求：[去重/去噪/格式转换]
- 增强需求：[AI分析/统计/可视化]

### 数据输出
- 渠道1：[优先级P0] [格式] [用途]
- 渠道2：[优先级P1] [格式] [用途]

---

## 4️⃣ 核心功能（MVP）

### 功能1: [功能名称]
**输入**: 
**输出**: 
**验收标准**: 
**测试方法**: 

### 功能2: [功能名称]
[同上]

---

## 5️⃣ 扩展功能（可选）

按优先级排序：

- [ ] P1: [功能描述]
- [ ] P2: [功能描述]
- [ ] P3: [功能描述]

---

## 6️⃣ 技术约束

### 已有的技术栈
- [ ] [技术1]（版本号，运行状态）
- [ ] [技术2]

### 技术偏好
- ✅ 偏好：[简单/模块化/无数据库]
- ❌ 避免：[复杂框架/重度依赖]

### 预算和时间
- 开发时间：[X周]
- 运行成本：[¥X/月]
- 维护成本：[多少时间]

---

## 7️⃣ 参考示例

### 竞品/灵感来源
- [链接1] - [说明]
- [截图] - [重点关注什么]

### 不想要的反例
- [示例] - [问题是什么]

---

## 8️⃣ 不确定的点

需要帮助决策的地方：

### 技术选型
❓ [问题1]
🤔 [你的初步想法]

❓ [问题2]
🤔 [你的初步想法]

### 实现方案
❓ [问题3]
🤔 [备选方案A vs 方案B]

---

## 9️⃣ 验收标准

### P0：必须满足
- [ ] [标准1]
- [ ] [标准2]

### P1：期望满足
- [ ] [标准3]
- [ ] [标准4]

### P2：锦上添花
- [ ] [标准5]

---

## 🔟 风险和备选方案

### 已知风险
**风险1**: [描述]
- 应对：[Plan A] 或 [Plan B]

**风险2**: [描述]
- 应对：[方案]
```

---

## 🚀 快速检查清单

开始项目前，用这个清单自检：

```markdown
## 需求是否清晰？

- [ ] 一句话能说清楚目标
- [ ] 知道数据从哪来，什么格式
- [ ] 知道数据要输出到哪里
- [ ] 定义了验收标准（什么算成功）
- [ ] 区分了MVP和扩展功能
- [ ] 列出了技术约束和偏好
- [ ] 提供了参考示例或截图
- [ ] 说明了不确定的点

## 技术细节是否充分？

- [ ] 现有系统的运行状态（端口/版本/配置）
- [ ] 已测试过关键接口（如RSS、API）
- [ ] 知道技术栈的坑和限制
- [ ] API账号和权限已准备好
- [ ] 预算和时间是明确的

## 是否可拆解验证？

- [ ] 能拆成3-5个独立模块
- [ ] 每个模块有明确的输入输出
- [ ] 每个模块可以单独测试
- [ ] 模块之间的依赖关系清晰
```

**如果以上80%都是✅，可以开始开发了！**

---

## 💎 总结：10条黄金法则

### 1. **一句话目标优先**
不要说"我想做XXX功能"，而要说"我要解决XXX问题"

### 2. **数据流思维**
从数据来源 → 处理 → 输出，理清整个链路

### 3. **优先级明确**
P0（必须）、P1（重要）、P2（可选），不要全部混在一起

### 4. **模块化拆解**
3-5个独立模块，每个都能单独测试

### 5. **验收标准量化**
不要说"好用"，要说"80%推荐准确，成本<¥0.1/次"

### 6. **技术细节充分**
端口、版本、配置、已测试的接口，越详细越好

### 7. **提供参考示例**
截图、链接、竞品，让AI知道"长什么样"

### 8. **说明不确定的点**
哪些需要AI帮忙决策，哪些你已经想好了

### 9. **分阶段推进**
第1周MVP，第2周完善，第3周优化，不要一次做完

### 10. **用清单自检**
开始前过一遍检查清单，80%✅再动手

---

## 🎓 案例对比

### ❌ 模糊需求

> "我想做一个爬虫，爬公众号文章，用AI分析，存到飞书"

**问题**:
- 不知道数据源状态
- 不知道AI要分析什么
- 不知道"存飞书"是指哪里
- 没有优先级
- 没有验收标准

**预计耗时**: 40-60轮对话，多次返工

---

### ✅ 清晰需求

> "我的目标：节省每天2小时刷公众号的时间
> 
> 数据源：已有wechat2rss服务（localhost:8081），20个公众号RSS订阅
> 
> 期望产物：
> - P0：AI每日报告（格式见截图），推送到飞书群 oc_xxx
> - P1：原始文章存飞书多维表格（方便检索）
> 
> 技术约束：
> - 不用数据库
> - AI成本<¥5/月
> - 2周内上线MVP
> 
> 不确定的点：
> - HTML转Markdown？还是纯文本？
> - AI用哪家（Claude/OpenAI/DeepSeek）？
> - 图片要保留吗？
> 
> 参考：见 pics/image.png（期望的报告格式）"

**预计耗时**: 15-20轮对话，线性推进

---

## 📚 延伸阅读

### 需求工程相关
- 《用户故事地图》 - Jeff Patton
- 《精益创业》 - Eric Ries
- 《Need-Finding 方法论》 - Stanford d.school

### AI项目最佳实践
- OpenAI API Best Practices
- Anthropic Prompt Engineering Guide
- Google AI Project Checklist

### 项目管理
- Agile MVP 方法
- SMART 目标设定法
- 5W2H 分析法

---

## 📊 附录：本项目的实际数据

### 对话统计
- 总对话轮数：~50轮
- 核心功能开发：30轮
- 问题排查修复：15轮
- 文档和优化：5轮

### 返工记录
1. RSS地址配置（192.168.0.121 → localhost）
2. 函数参数修复（days_ago → filter_24h）
3. 清洗模块重构（v1纯文本 → v2 Markdown）
4. 字段名统一（url/link, publish_time/published）
5. 飞书权限调试（3次）
6. URL字段格式修正
7. 空值验证添加

### 需求变更
1. "存飞书" → 明确为飞书群推送
2. 新增飞书多维表格存储
3. 单一模式 → 三种工作模式（bitable/group/both）

### 最终交付
- 代码文件：10个核心模块
- 配置文件：4个
- 文档文件：4个（含本复盘）
- 测试文件：5个（已删除）
- 总代码量：~2000行

---

**创建日期**: 2025-12-29
**适用场景**: AI辅助开发项目、需求梳理、项目启动
**维护者**: 项目团队

---

> 💡 **记住**: 好的需求梳理是成功的一半。花20%的时间理清需求，可以节省80%的返工时间。

