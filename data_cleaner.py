"""
æ•°æ®æ¸…æ´—æ¨¡å— V2 - ä½¿ç”¨Markdownæ ¼å¼ï¼ˆæ¨èï¼‰
"""

from bs4 import BeautifulSoup
import markdownify
import re


def clean_html_to_markdown(html_content, keep_images='full'):
    """
    å°†HTMLæ¸…æ´—å¹¶è½¬æ¢ä¸ºMarkdownæ ¼å¼
    
    Args:
        html_content: HTMLå†…å®¹
        keep_images: å›¾ç‰‡å¤„ç†ç­–ç•¥
            - 'full': ä¿ç•™å®Œæ•´å›¾ç‰‡é“¾æ¥ï¼ˆé»˜è®¤ï¼‰
            - 'simplified': ç®€åŒ–ä¸º[å›¾ç‰‡N]
            - 'remove': å®Œå…¨ç§»é™¤å›¾ç‰‡
    
    Returns:
        Markdownæ ¼å¼çš„æ–‡æœ¬
    """
    if not html_content:
        return ""
    
    try:
        # æ­¥éª¤1ï¼šç”¨BeautifulSoupæ¸…ç†åƒåœ¾æ ‡ç­¾
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # ç§»é™¤scriptã€styleã€iframeç­‰æ— ç”¨æ ‡ç­¾
        for tag in soup(['script', 'style', 'iframe', 'noscript']):
            tag.decompose()
        
        # æ­¥éª¤2ï¼šè½¬æ¢ä¸ºMarkdown
        markdown = markdownify.markdownify(
            str(soup),
            heading_style='ATX',        # ä½¿ç”¨ # é£æ ¼çš„æ ‡é¢˜
            bullets='*',                 # ä½¿ç”¨ * ä½œä¸ºåˆ—è¡¨ç¬¦å·
            strip=['a'],                 # å¯é€‰ï¼šç§»é™¤é“¾æ¥ä½†ä¿ç•™æ–‡æœ¬
        )
        
        # æ­¥éª¤3ï¼šå¤„ç†å›¾ç‰‡
        if keep_images == 'remove':
            # å®Œå…¨ç§»é™¤å›¾ç‰‡
            markdown = re.sub(r'!\[([^\]]*)\]\([^\)]+\)', '', markdown)
        elif keep_images == 'simplified':
            # ç®€åŒ–ä¸º[å›¾ç‰‡N]
            img_count = 0
            def replace_img(match):
                nonlocal img_count
                img_count += 1
                return f'[å›¾ç‰‡{img_count}]'
            markdown = re.sub(r'!\[([^\]]*)\]\([^\)]+\)', replace_img, markdown)
        # else: keep_images == 'full', ä¿æŒåŸæ ·
        
        # æ­¥éª¤4ï¼šæ¸…ç†å¤šä½™çš„ç©ºè¡Œ
        lines = markdown.split('\n')
        cleaned_lines = []
        prev_empty = False
        
        for line in lines:
            is_empty = not line.strip()
            # æœ€å¤šä¿ç•™ä¸€ä¸ªç©ºè¡Œ
            if is_empty:
                if not prev_empty:
                    cleaned_lines.append(line)
                prev_empty = True
            else:
                cleaned_lines.append(line)
                prev_empty = False
        
        return '\n'.join(cleaned_lines).strip()
        
    except Exception as e:
        print(f"âš ï¸  HTMLè½¬Markdownå¤±è´¥: {e}")
        # é™çº§å¤„ç†ï¼šç›´æ¥å»é™¤HTMLæ ‡ç­¾
        return re.sub(r'<[^>]+>', '', html_content)


def remove_ads_markdown(markdown_text):
    """
    ä»Markdownæ–‡æœ¬ä¸­å»é™¤å¹¿å‘Šå†…å®¹
    
    Args:
        markdown_text: Markdownæ–‡æœ¬
    
    Returns:
        å»é™¤å¹¿å‘Šåçš„Markdownæ–‡æœ¬
    """
    # å¹¿å‘Šå…³é”®è¯åˆ—è¡¨
    ad_keywords = [
        'æ‰«ç å…³æ³¨', 'é•¿æŒ‰äºŒç»´ç ', 'è¯†åˆ«äºŒç»´ç ', 'å…³æ³¨å…¬ä¼—å·',
        'ç‚¹å‡»é˜…è¯»åŸæ–‡', 'é˜…è¯»åŸæ–‡', 'åŸæ–‡é“¾æ¥',
        'é™æ—¶ä¼˜æƒ ', 'é™æ—¶ç‰¹æƒ ', 'æŠ¥åé“¾æ¥', 'ç‚¹å‡»æŠ¥å',
        'åŠ å¾®ä¿¡', 'æ·»åŠ å¾®ä¿¡', 'å¾®ä¿¡å’¨è¯¢',
        'æ¨å¹¿', 'å¹¿å‘Š', 'èµåŠ©',
        'è½¬å‘æœ‹å‹åœˆ', 'åˆ†äº«åˆ°æœ‹å‹åœˆ',
        'ç‚¹å‡»è´­ä¹°', 'ç«‹å³è´­ä¹°', 'é©¬ä¸Šè´­ä¹°',
        'è¯¾ç¨‹é“¾æ¥', 'è´­ä¹°é“¾æ¥',
        'è·³è½¬å¾®ä¿¡æ‰“å¼€',  # RSSç‰¹æœ‰çš„
    ]
    
    # æŒ‰è¡Œå¤„ç†
    lines = markdown_text.split('\n')
    cleaned_lines = []
    
    for line in lines:
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¹¿å‘Šå…³é”®è¯
        has_ad = False
        for keyword in ad_keywords:
            if keyword in line:
                has_ad = True
                break
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å¾®ä¿¡å·/ç”µè¯å·ç 
        if re.search(r'å¾®ä¿¡[ï¼š:]\s*[a-zA-Z0-9_-]+', line):
            has_ad = True
        if re.search(r'ç”µè¯[ï¼š:]\s*\d{11}', line):
            has_ad = True
        
        # è¿‡æ»¤å›¾ç‰‡é“¾æ¥ï¼ˆå¯é€‰ï¼‰
        # if line.strip().startswith('![](') and 'img-proxy' in line:
        #     has_ad = True
        
        # å¦‚æœä¸æ˜¯å¹¿å‘Šï¼Œä¿ç•™è¿™ä¸€è¡Œ
        if not has_ad:
            cleaned_lines.append(line)
    
    return '\n'.join(cleaned_lines)


def calculate_word_count_markdown(markdown_text):
    """
    è®¡ç®—Markdownæ–‡æœ¬çš„å­—æ•°ï¼ˆå»é™¤Markdownæ ‡è®°ï¼‰
    
    Args:
        markdown_text: Markdownæ–‡æœ¬
    
    Returns:
        å­—æ•°
    """
    if not markdown_text:
        return 0
    
    # ç§»é™¤Markdownæ ‡è®°
    # ç§»é™¤æ ‡é¢˜æ ‡è®° (# ## ###)
    text = re.sub(r'^#+\s+', '', markdown_text, flags=re.MULTILINE)
    
    # ç§»é™¤åŠ ç²—/æ–œä½“æ ‡è®° (** __ * _)
    text = re.sub(r'\*\*([^*]+)\*\*', r'\1', text)
    text = re.sub(r'__([^_]+)__', r'\1', text)
    text = re.sub(r'\*([^*]+)\*', r'\1', text)
    text = re.sub(r'_([^_]+)_', r'\1', text)
    
    # ç§»é™¤é“¾æ¥ [text](url)
    text = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', text)
    
    # ç§»é™¤å›¾ç‰‡ ![alt](url)
    text = re.sub(r'!\[([^\]]*)\]\([^\)]+\)', '', text)
    
    # ç§»é™¤åˆ—è¡¨æ ‡è®° (* - 1.)
    text = re.sub(r'^\s*[\*\-\+]\s+', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*\d+\.\s+', '', text, flags=re.MULTILINE)
    
    # ç§»é™¤å¼•ç”¨æ ‡è®° (>)
    text = re.sub(r'^>\s+', '', text, flags=re.MULTILINE)
    
    # ç§»é™¤ä»£ç å—æ ‡è®° (```)
    text = re.sub(r'```[^\n]*\n.*?```', '', text, flags=re.DOTALL)
    text = re.sub(r'`([^`]+)`', r'\1', text)
    
    # å»é™¤æ‰€æœ‰ç©ºç™½å­—ç¬¦åç»Ÿè®¡
    text_no_space = re.sub(r'\s+', '', text)
    return len(text_no_space)


def deduplicate_articles(articles):
    """
    å»é‡ï¼ˆåŸºäºURLå’Œæ ‡é¢˜ï¼‰
    
    Args:
        articles: æ–‡ç« åˆ—è¡¨
    
    Returns:
        å»é‡åçš„æ–‡ç« åˆ—è¡¨
    """
    seen = set()
    unique_articles = []
    
    for article in articles:
        # ä½¿ç”¨ URL æˆ– (author, title) ç»„åˆä½œä¸ºå”¯ä¸€é”®
        key = article.get('url', '')
        if not key:
            key = (article.get('author', ''), article.get('title', ''))
        
        if key not in seen:
            seen.add(key)
            unique_articles.append(article)
    
    duplicate_count = len(articles) - len(unique_articles)
    if duplicate_count > 0:
        print(f"   ğŸ”„ å»é‡: ç§»é™¤ {duplicate_count} ç¯‡é‡å¤æ–‡ç« ")
    
    return unique_articles


def filter_low_quality(articles, min_word_count=500):
    """
    è¿‡æ»¤ä½è´¨é‡æ–‡ç« 
    
    Args:
        articles: æ–‡ç« åˆ—è¡¨
        min_word_count: æœ€å°å­—æ•°
    
    Returns:
        è¿‡æ»¤åçš„æ–‡ç« åˆ—è¡¨
    """
    filtered = []
    removed_count = 0
    
    for article in articles:
        # æ£€æŸ¥å­—æ•°
        if article.get('word_count', 0) < min_word_count:
            removed_count += 1
            continue
        
        # æ£€æŸ¥æ ‡é¢˜
        title = article.get('title', '').lower()
        if 'æµ‹è¯•' in title or 'test' in title:
            removed_count += 1
            continue
        
        # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºç©º
        content = article.get('content_markdown', '')
        if not content or len(content.strip()) < 100:
            removed_count += 1
            continue
        
        filtered.append(article)
    
    if removed_count > 0:
        print(f"   ğŸ—‘ï¸  è¿‡æ»¤: ç§»é™¤ {removed_count} ç¯‡ä½è´¨é‡æ–‡ç« ")
    
    return filtered


def clean_articles_v2(articles, min_word_count=500):
    """
    æ¸…æ´—æ–‡ç« æ•°æ®çš„ä¸»å‡½æ•°ï¼ˆMarkdownç‰ˆæœ¬ï¼‰
    
    Args:
        articles: åŸå§‹æ–‡ç« åˆ—è¡¨
        min_word_count: æœ€å°å­—æ•°é˜ˆå€¼
    
    Returns:
        æ¸…æ´—åçš„æ–‡ç« åˆ—è¡¨ï¼ˆåŒ…å«Markdownæ ¼å¼ï¼‰
    """
    print("\n" + "=" * 60)
    print("ğŸ§¹ å¼€å§‹æ¸…æ´—æ•°æ®ï¼ˆMarkdownæ ¼å¼ï¼‰")
    print("=" * 60)
    
    print(f"\nåŸå§‹æ–‡ç« æ•°: {len(articles)}")
    
    # 1. è½¬æ¢ä¸ºMarkdownå¹¶å»é™¤å¹¿å‘Š
    print("\n1ï¸âƒ£  è½¬æ¢ä¸ºMarkdownæ ¼å¼...")
    for article in articles:
        # è½¬æ¢HTMLä¸ºMarkdown
        html_content = article.get('content_html', '')
        markdown = clean_html_to_markdown(html_content)
        
        # å»é™¤å¹¿å‘Š
        markdown = remove_ads_markdown(markdown)
        
        # ä¿å­˜Markdown
        article['content_markdown'] = markdown
        
        # è®¡ç®—å­—æ•°
        article['word_count'] = calculate_word_count_markdown(markdown)
        
        # å¤„ç†æ‘˜è¦
        if not article.get('summary'):
            # å¦‚æœæ²¡æœ‰æ‘˜è¦ï¼Œå–å‰200å­—ï¼ˆå»é™¤Markdownæ ‡è®°ï¼‰
            plain_text = re.sub(r'[#*_\[\]()>]', '', markdown)
            article['summary'] = plain_text[:200] + '...' if len(plain_text) > 200 else plain_text
    
    print(f"   âœ… Markdownè½¬æ¢å®Œæˆ")
    
    # 2. å»é‡
    print("\n2ï¸âƒ£  å»é™¤é‡å¤æ–‡ç« ...")
    articles = deduplicate_articles(articles)
    print(f"   âœ… å½“å‰æ–‡ç« æ•°: {len(articles)}")
    
    # 3. è¿‡æ»¤ä½è´¨é‡
    print(f"\n3ï¸âƒ£  è¿‡æ»¤ä½è´¨é‡æ–‡ç« ï¼ˆæœ€å°å­—æ•°: {min_word_count}ï¼‰...")
    articles = filter_low_quality(articles, min_word_count)
    print(f"   âœ… å½“å‰æ–‡ç« æ•°: {len(articles)}")
    
    # 4. ç»Ÿè®¡
    print("\n" + "=" * 60)
    print("âœ… æ•°æ®æ¸…æ´—å®Œæˆï¼")
    print(f"   æœ€ç»ˆæ–‡ç« æ•°: {len(articles)}")
    
    if articles:
        total_words = sum(a['word_count'] for a in articles)
        avg_words = total_words // len(articles)
        print(f"   å¹³å‡å­—æ•°: {avg_words}")
        print(f"   å­—æ•°èŒƒå›´: {min(a['word_count'] for a in articles)} - {max(a['word_count'] for a in articles)}")
    
    return articles


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("æµ‹è¯•HTML â†’ Markdownè½¬æ¢:")
    test_html = """
    <div class="article">
        <h1>AIå·¥å…·æ•™ç¨‹</h1>
        <p>è¿™æ˜¯ä¸€æ®µ<strong>é‡è¦</strong>çš„å†…å®¹ã€‚</p>
        <ul>
            <li>Claudeï¼šæœ€å¼ºçš„AIåŠ©æ‰‹</li>
            <li>GPT-4ï¼šOpenAIçš„æ——èˆ°æ¨¡å‹</li>
        </ul>
        <script>console.log('should be removed')</script>
        <p>æ‰«ç å…³æ³¨æˆ‘ä»¬çš„å…¬ä¼—å·</p>
        <p>å¾®ä¿¡ï¼štest123456</p>
        <p>è¿™æ˜¯æ­£å¸¸å†…å®¹ã€‚</p>
    </div>
    """
    
    markdown = clean_html_to_markdown(test_html)
    markdown = remove_ads_markdown(markdown)
    
    print(markdown)
    print(f"\nå­—æ•°: {calculate_word_count_markdown(markdown)}")

